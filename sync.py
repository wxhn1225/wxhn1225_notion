import requests
import json
import os
import base64
import hashlib
from datetime import datetime, timedelta
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

NOTION_API_KEY = os.getenv('NOTION_API_KEY')
# æ”¯æŒå¤šä¸ªæ•°æ®åº“IDï¼ˆä¼˜å…ˆä½¿ç”¨NOTION_DATABASE_IDSï¼‰
NOTION_DATABASE_IDS = os.getenv('NOTION_DATABASE_IDS')
NOTION_DATABASE_ID = os.getenv('NOTION_DATABASE_ID')  # å…¼å®¹æ—§ç‰ˆæœ¬
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')
GITHUB_REPO = os.getenv('GITHUB_REPO')
GITHUB_OWNER = os.getenv('GITHUB_OWNER')
GITHUB_PATH = os.getenv('GITHUB_PATH', 'notes')

# åŒæ­¥æ¨¡å¼é…ç½®
SYNC_MODE = os.getenv('SYNC_MODE', 'all')  # 'databases', 'pages', 'all'
BATCH_COMMIT = os.getenv('BATCH_COMMIT', 'true').lower() == 'true'  # æ˜¯å¦æ‰¹é‡æäº¤
SKIP_COMMIT = os.getenv('SKIP_COMMIT', 'false').lower() == 'true'  # æ˜¯å¦è·³è¿‡æäº¤

# æ–‡ä»¶å¤¹åˆ†ç±»é…ç½®
CATEGORY_PROPERTIES = os.getenv('CATEGORY_PROPERTIES', 'Status,Category,Type,çŠ¶æ€,åˆ†ç±»,ç±»å‹,Stage,é˜¶æ®µ').split(',')
ENABLE_CATEGORIZATION = os.getenv('ENABLE_CATEGORIZATION', 'true').lower() == 'true'  # æ˜¯å¦å¯ç”¨åˆ†ç±»

# å­˜å‚¨å¾…æäº¤çš„æ–‡ä»¶
pending_files = []

# æ–‡ä»¶ä½ç½®æ˜ å°„è¡¨æ–‡ä»¶
MAPPING_FILE = 'file_mapping.json'


def load_file_mapping():
    """åŠ è½½æ–‡ä»¶ä½ç½®æ˜ å°„è¡¨"""
    try:
        if os.path.exists(MAPPING_FILE):
            with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    except Exception as e:
        print(f"âš ï¸ åŠ è½½æ–‡ä»¶æ˜ å°„è¡¨æ—¶å‡ºé”™: {e}")
        return {}


def save_file_mapping(mapping):
    """ä¿å­˜æ–‡ä»¶ä½ç½®æ˜ å°„è¡¨"""
    try:
        with open(MAPPING_FILE, 'w', encoding='utf-8') as f:
            json.dump(mapping, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ–‡ä»¶æ˜ å°„è¡¨æ—¶å‡ºé”™: {e}")


def delete_github_file(file_path):
    """åˆ é™¤GitHubä¸Šçš„æ–‡ä»¶"""
    headers = {
        'Authorization': f'token {GITHUB_TOKEN}',
        'Accept': 'application/vnd.github.v3+json'
    }

    url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{file_path}'

    try:
        # å…ˆè·å–æ–‡ä»¶ä¿¡æ¯ä»¥è·å–SHA
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            file_data = response.json()
            sha = file_data['sha']

            # åˆ é™¤æ–‡ä»¶
            delete_data = {
                'message': f'ğŸ—‘ï¸ æ¸…ç†æ—§ä½ç½®æ–‡ä»¶: {file_path}',
                'sha': sha
            }

            delete_response = requests.delete(url, headers=headers, json=delete_data)
            if delete_response.status_code == 200:
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ–‡ä»¶: {file_path}")
                return True
            else:
                print(f"âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥: {file_path} - {delete_response.status_code}")
        else:
            print(f"ğŸ“ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤: {file_path}")
        
        return False
    except Exception as e:
        print(f"âš ï¸ åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™: {file_path} - {e}")
        return False


def clean_deleted_pages(file_mapping):
    """æ¸…ç†å·²åˆ é™¤é¡µé¢å¯¹åº”çš„GitHubæ–‡ä»¶"""
    if not file_mapping:
        return

    print(f"\nğŸ§¹ æ£€æŸ¥å·²åˆ é™¤çš„é¡µé¢...")
    
    # è·å–å½“å‰åŒæ­¥ä¸­å¤„ç†è¿‡çš„é¡µé¢ID
    current_session_pages = set()
    
    # ä»pending_filesä¸­æå–é¡µé¢IDï¼ˆè¿™éœ€è¦æˆ‘ä»¬å­˜å‚¨é¡µé¢IDä¿¡æ¯ï¼‰
    # ç”±äºå½“å‰ç»“æ„é™åˆ¶ï¼Œæˆ‘ä»¬å…ˆè·³è¿‡è¿™ä¸ªåŠŸèƒ½ï¼Œåœ¨åç»­ç‰ˆæœ¬ä¸­æ”¹è¿›
    
    # TODO: åœ¨åç»­ç‰ˆæœ¬ä¸­æ·»åŠ æ›´å®Œå–„çš„æ¸…ç†æœºåˆ¶
    # ç›®å‰åªæ¸…ç†ä½ç½®å˜æ›´çš„æ–‡ä»¶ï¼Œå·²åˆ é™¤é¡µé¢çš„æ¸…ç†å°†åœ¨åç»­ç‰ˆæœ¬ä¸­å®ç°
    
    print(f"âœ… æ¸…ç†æ£€æŸ¥å®Œæˆ")


def get_database_ids():
    """è·å–è¦åŒæ­¥çš„æ•°æ®åº“IDåˆ—è¡¨"""
    if NOTION_DATABASE_IDS:
        # æ”¯æŒå¤šä¸ªæ•°æ®åº“IDï¼Œç”¨é€—å·åˆ†éš”
        database_ids = [db_id.strip() for db_id in NOTION_DATABASE_IDS.split(',') if db_id.strip()]
        return database_ids
    elif NOTION_DATABASE_ID:
        # å…¼å®¹å•ä¸ªæ•°æ®åº“ID
        return [NOTION_DATABASE_ID.strip()]
    else:
        return []


def search_all_pages():
    """æœç´¢æ‰€æœ‰é¡µé¢ï¼ˆåŒ…æ‹¬æ•°æ®åº“ä¸­çš„é¡µé¢å’Œç‹¬ç«‹é¡µé¢ï¼‰"""
    headers = {
        'Authorization': f'Bearer {NOTION_API_KEY}',
        'Content-Type': 'application/json',
        'Notion-Version': '2022-06-28'
    }

    url = 'https://api.notion.com/v1/search'

    all_pages = []
    has_more = True
    start_cursor = None

    while has_more:
        data = {
            'filter': {
                'property': 'object',
                'value': 'page'
            },
            'page_size': 100
        }

        if start_cursor:
            data['start_cursor'] = start_cursor

        try:
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()
            result = response.json()

            all_pages.extend(result.get('results', []))
            has_more = result.get('has_more', False)
            start_cursor = result.get('next_cursor')

        except requests.exceptions.RequestException as e:
            print(f"æœç´¢é¡µé¢æ—¶å‡ºé”™: {e}")
            break

    return all_pages


def get_database_info(database_id):
    """è·å–æ•°æ®åº“ä¿¡æ¯ï¼ˆåŒ…æ‹¬åç§°ï¼‰"""
    headers = {
        'Authorization': f'Bearer {NOTION_API_KEY}',
        'Content-Type': 'application/json',
        'Notion-Version': '2022-06-28'
    }

    url = f'https://api.notion.com/v1/databases/{database_id}'

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        db_data = response.json()

        # è·å–æ•°æ®åº“æ ‡é¢˜
        db_title = "æœªå‘½åæ•°æ®åº“"
        if 'title' in db_data and db_data['title']:
            db_title = db_data['title'][0]['plain_text']



        return {
            'id': database_id,
            'title': db_title,
            'data': db_data
        }
    except requests.exceptions.RequestException as e:
        print(f"è·å–æ•°æ®åº“ {database_id} ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return {
            'id': database_id,
            'title': f"æ•°æ®åº“_{database_id[:8]}",
            'data': None
        }


def clean_folder_name(name):
    """æ¸…ç†æ–‡ä»¶å¤¹åç§°ï¼Œç§»é™¤ä¸åˆæ³•å­—ç¬¦"""
    # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•çš„æ–‡ä»¶å¤¹å­—ç¬¦
    invalid_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
    for char in invalid_chars:
        name = name.replace(char, '_')

    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    name = name.strip(' .')

    # å¦‚æœåç§°ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
    if not name:
        name = "æœªå‘½å"

    return name


def parse_date_string(date_str):
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    if not date_str:
        return None
    
    # å°è¯•è§£æISOæ ¼å¼æ—¥æœŸ
    try:
        if 'T' in date_str:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00')).date()
        else:
            return datetime.fromisoformat(date_str).date()
    except:
        pass
    
    # å°è¯•è§£æå…¶ä»–å¸¸è§æ ¼å¼
    date_formats = [
        '%Y-%m-%d',
        '%Y/%m/%d',
        '%Yå¹´%mæœˆ%dæ—¥',
        '%m/%d/%Y',
        '%d/%m/%Y'
    ]
    
    for fmt in date_formats:
        try:
            return datetime.strptime(date_str, fmt).date()
        except:
            continue
    
    return None


def get_week_range(date_obj):
    """è·å–æŒ‡å®šæ—¥æœŸæ‰€åœ¨å‘¨çš„æ—¥æœŸèŒƒå›´ï¼ˆå‘¨æ—¥åˆ°å‘¨å…­ï¼‰"""
    if not date_obj:
        return None
    
    # è·å–å‘¨æ—¥ï¼ˆweekday() è¿”å›0-6ï¼Œ0æ˜¯å‘¨ä¸€ï¼Œ6æ˜¯å‘¨æ—¥ï¼‰
    # å¦‚æœä»Šå¤©æ˜¯å‘¨æ—¥(weekday=6)ï¼Œåˆ™todayå°±æ˜¯å‘¨æ—¥èµ·å§‹
    # å¦åˆ™éœ€è¦å›é€€åˆ°ä¸Šä¸€ä¸ªå‘¨æ—¥
    days_since_sunday = (date_obj.weekday() + 1) % 7
    sunday = date_obj - timedelta(days=days_since_sunday)
    # è·å–å‘¨å…­
    saturday = sunday + timedelta(days=6)
    
    return (sunday, saturday)


def format_week_range(start_date, end_date):
    """æ ¼å¼åŒ–å‘¨æœŸèŒƒå›´ä¸ºæ–‡ä»¶å¤¹åç§°"""
    if not start_date or not end_date:
        return None
    
    # å¦‚æœåœ¨åŒä¸€ä¸ªæœˆ
    if start_date.month == end_date.month:
        return f"{start_date.year}å¹´{start_date.month}æœˆ{start_date.day}æ—¥-{end_date.day}æ—¥"
    # å¦‚æœè·¨æœˆ
    elif start_date.year == end_date.year:
        return f"{start_date.year}å¹´{start_date.month}æœˆ{start_date.day}æ—¥-{end_date.month}æœˆ{end_date.day}æ—¥"
    # å¦‚æœè·¨å¹´
    else:
        return f"{start_date.year}å¹´{start_date.month}æœˆ{start_date.day}æ—¥-{end_date.year}å¹´{end_date.month}æœˆ{end_date.day}æ—¥"


def generate_date_category(date_value):
    """æ ¹æ®æ—¥æœŸå€¼ç”Ÿæˆåˆ†ç±»æ–‡ä»¶å¤¹åç§°"""
    if not date_value:
        return None
    
    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆè§£æä¸ºæ—¥æœŸå¯¹è±¡
    if isinstance(date_value, str):
        date_obj = parse_date_string(date_value)
    else:
        date_obj = date_value
    
    if not date_obj:
        return None
    
    # è·å–å‘¨æœŸèŒƒå›´
    week_range = get_week_range(date_obj)
    if week_range:
        start_date, end_date = week_range
        return format_week_range(start_date, end_date)
    
    return None


def fetch_notion_notes(database_id):
    """è·å–æŒ‡å®šNotionæ•°æ®åº“ä¸­çš„ç¬”è®°"""
    headers = {
        'Authorization': f'Bearer {NOTION_API_KEY}',
        'Content-Type': 'application/json',
        'Notion-Version': '2022-06-28'
    }

    url = f'https://api.notion.com/v1/databases/{database_id}/query'

    try:
        response = requests.post(url, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è·å–æ•°æ®åº“ {database_id} çš„ç¬”è®°æ—¶å‡ºé”™: {e}")
        return None


def get_page_content(page_id):
    """è·å–é¡µé¢çš„å…·ä½“å†…å®¹"""
    headers = {
        'Authorization': f'Bearer {NOTION_API_KEY}',
        'Content-Type': 'application/json',
        'Notion-Version': '2022-06-28'
    }

    url = f'https://api.notion.com/v1/blocks/{page_id}/children'

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"è·å–é¡µé¢å†…å®¹æ—¶å‡ºé”™: {e}")
        return None


def get_page_title(page_data):
    """ä»é¡µé¢æ•°æ®ä¸­æå–æ ‡é¢˜"""
    if 'properties' in page_data:
        for prop_name, prop_data in page_data['properties'].items():
            if prop_data['type'] == 'title' and 'title' in prop_data:
                if prop_data['title']:
                    return prop_data['title'][0]['plain_text']
                break

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°titleå±æ€§ï¼Œå°è¯•ä»å…¶ä»–åœ°æ–¹è·å–
    if 'title' in page_data and page_data['title']:
        return page_data['title'][0]['plain_text'] if isinstance(page_data['title'], list) else str(page_data['title'])

    return ""


def get_page_properties(page_data):
    """ä»é¡µé¢æ•°æ®ä¸­æå–æ‰€æœ‰å±æ€§"""
    properties = {}
    if 'properties' in page_data:
        for prop_name, prop_data in page_data['properties'].items():
            prop_type = prop_data.get('type', '')
            
            if prop_type == 'select' and 'select' in prop_data and prop_data['select']:
                properties[prop_name] = prop_data['select']['name']
            elif prop_type == 'multi_select' and 'multi_select' in prop_data:
                properties[prop_name] = [item['name'] for item in prop_data['multi_select']]
            elif prop_type == 'status' and 'status' in prop_data and prop_data['status']:
                properties[prop_name] = prop_data['status']['name']
            elif prop_type == 'rich_text' and 'rich_text' in prop_data and prop_data['rich_text']:
                properties[prop_name] = prop_data['rich_text'][0]['plain_text']
            elif prop_type == 'number' and 'number' in prop_data and prop_data['number'] is not None:
                properties[prop_name] = str(prop_data['number'])
            elif prop_type == 'checkbox' and 'checkbox' in prop_data:
                properties[prop_name] = 'å·²å®Œæˆ' if prop_data['checkbox'] else 'æœªå®Œæˆ'
            elif prop_type == 'date' and 'date' in prop_data and prop_data['date']:
                properties[prop_name] = prop_data['date']['start']
            elif prop_type == 'formula' and 'formula' in prop_data:
                # è·å–å…¬å¼è®¡ç®—ç»“æœ
                formula_result = prop_data['formula']
                if formula_result.get('type') == 'string' and formula_result.get('string'):
                    properties[prop_name] = formula_result['string']
                elif formula_result.get('type') == 'number' and formula_result.get('number') is not None:
                    properties[prop_name] = str(formula_result['number'])
                elif formula_result.get('type') == 'date' and formula_result.get('date'):
                    properties[prop_name] = formula_result['date']['start']
            elif prop_type == 'rollup' and 'rollup' in prop_data:
                # è·å–æ±‡æ€»ç»“æœ
                rollup_result = prop_data['rollup']
                if rollup_result.get('type') == 'array' and rollup_result.get('array'):
                    # å¤„ç†æ•°ç»„ç±»å‹çš„æ±‡æ€»ç»“æœ
                    array_values = []
                    for item in rollup_result['array']:
                        if item.get('type') == 'rich_text' and item.get('rich_text'):
                            array_values.append(item['rich_text'][0]['plain_text'])
                    if array_values:
                        properties[prop_name] = array_values
                elif rollup_result.get('type') == 'number' and rollup_result.get('number') is not None:
                    properties[prop_name] = str(rollup_result['number'])
            elif prop_type == 'created_time':
                properties[prop_name] = prop_data['created_time']
            elif prop_type == 'last_edited_time':
                properties[prop_name] = prop_data['last_edited_time']
    
    return properties


def generate_folder_path(database_title, page_properties):
    """æ ¹æ®æ•°æ®åº“æ ‡é¢˜å’Œé¡µé¢å±æ€§ç”Ÿæˆæ–‡ä»¶å¤¹è·¯å¾„"""
    base_folder = clean_folder_name(database_title)
    
    # å¦‚æœç¦ç”¨åˆ†ç±»ï¼Œç›´æ¥è¿”å›åŸºç¡€æ–‡ä»¶å¤¹
    if not ENABLE_CATEGORIZATION:
        return base_folder
    
    # æŸ¥æ‰¾åˆ†ç±»å±æ€§
    category_value = None
    category_prop_name = None
    for prop_name in CATEGORY_PROPERTIES:
        prop_name = prop_name.strip()  # ç§»é™¤ç©ºæ ¼
        if prop_name in page_properties:
            category_value = page_properties[prop_name]
            category_prop_name = prop_name
            break
    
    if category_value:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¥æœŸå±æ€§ï¼Œå¦‚æœæ˜¯åˆ™è¿›è¡Œç‰¹æ®Šå¤„ç†
        date_keywords = ['date', 'time', 'æ—¥æœŸ', 'æ—¶é—´', 'full date']
        is_date_prop = category_prop_name and any(date_keyword in category_prop_name.lower() 
                                                 for date_keyword in date_keywords)
        
        if is_date_prop:
            # å°è¯•ç”Ÿæˆæ—¥æœŸèŒƒå›´åˆ†ç±»
            date_category = generate_date_category(category_value)
            if date_category:
                category_folder = clean_folder_name(date_category)
                return f"{base_folder}/{category_folder}"
        
        # å¦‚æœä¸æ˜¯æ—¥æœŸå±æ€§æˆ–æ—¥æœŸè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å€¼
        category_folder = clean_folder_name(str(category_value))
        return f"{base_folder}/{category_folder}"
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†ç±»å±æ€§ï¼Œä½¿ç”¨åŸæ¥çš„æ–‡ä»¶å¤¹
        return base_folder


def convert_notion_to_markdown(page_data, content_data, source_info=""):
    """å°†Notioné¡µé¢è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    title = get_page_title(page_data)

    if not title:
        title = f"é¡µé¢_{page_data.get('id', 'unknown')}"

    markdown_content = f"# {title}\n\n"

    # æ·»åŠ æ¥æºä¿¡æ¯
    if source_info:
        markdown_content += f"**æ¥æº**: {source_info}\n\n"

    # æ·»åŠ åˆ›å»ºæ—¶é—´
    created_time = page_data.get('created_time', '')
    if created_time:
        markdown_content += f"**åˆ›å»ºæ—¶é—´**: {created_time}\n\n"

    # æ·»åŠ æœ€åç¼–è¾‘æ—¶é—´
    last_edited_time = page_data.get('last_edited_time', '')
    if last_edited_time:
        markdown_content += f"**æœ€åç¼–è¾‘**: {last_edited_time}\n\n"

    # æ·»åŠ åˆ†å‰²çº¿
    markdown_content += "---\n\n"

    # è½¬æ¢å†…å®¹å—
    if content_data and 'results' in content_data:
        for block in content_data['results']:
            markdown_content += convert_block_to_markdown(block)

    return markdown_content


def convert_block_to_markdown(block):
    """å°†å•ä¸ªNotionå—è½¬æ¢ä¸ºMarkdown"""
    block_type = block.get('type', '')

    if block_type == 'paragraph' and 'paragraph' in block:
        text = extract_text_from_rich_text(block['paragraph'].get('rich_text', []))
        return f"{text}\n\n"

    elif block_type == 'heading_1' and 'heading_1' in block:
        text = extract_text_from_rich_text(block['heading_1'].get('rich_text', []))
        return f"# {text}\n\n"

    elif block_type == 'heading_2' and 'heading_2' in block:
        text = extract_text_from_rich_text(block['heading_2'].get('rich_text', []))
        return f"## {text}\n\n"

    elif block_type == 'heading_3' and 'heading_3' in block:
        text = extract_text_from_rich_text(block['heading_3'].get('rich_text', []))
        return f"### {text}\n\n"

    elif block_type == 'bulleted_list_item' and 'bulleted_list_item' in block:
        text = extract_text_from_rich_text(block['bulleted_list_item'].get('rich_text', []))
        return f"- {text}\n"

    elif block_type == 'numbered_list_item' and 'numbered_list_item' in block:
        text = extract_text_from_rich_text(block['numbered_list_item'].get('rich_text', []))
        return f"1. {text}\n"

    elif block_type == 'code' and 'code' in block:
        text = extract_text_from_rich_text(block['code'].get('rich_text', []))
        language = block['code'].get('language', '')
        return f"```{language}\n{text}\n```\n\n"

    elif block_type == 'quote' and 'quote' in block:
        text = extract_text_from_rich_text(block['quote'].get('rich_text', []))
        return f"> {text}\n\n"

    elif block_type == 'callout' and 'callout' in block:
        text = extract_text_from_rich_text(block['callout'].get('rich_text', []))
        icon = block['callout'].get('icon', {})
        icon_text = ""
        if icon.get('type') == 'emoji':
            icon_text = icon.get('emoji', '') + " "
        return f"**{icon_text}æç¤º**: {text}\n\n"

    return ""


def extract_text_from_rich_text(rich_text_array):
    """ä»å¯Œæ–‡æœ¬æ•°ç»„ä¸­æå–çº¯æ–‡æœ¬"""
    text = ""
    for rich_text in rich_text_array:
        if 'plain_text' in rich_text:
            text += rich_text['plain_text']
    return text


def get_file_content_hash(content):
    """è®¡ç®—æ–‡ä»¶å†…å®¹çš„å“ˆå¸Œå€¼"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def get_existing_file_info(file_path):
    """è·å–GitHubä¸Šç°æœ‰æ–‡ä»¶çš„ä¿¡æ¯"""
    headers = {
        'Authorization': f'token {GITHUB_TOKEN}',
        'Accept': 'application/vnd.github.v3+json'
    }

    url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{file_path}'

    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            existing_data = response.json()
            # è§£ç ç°æœ‰æ–‡ä»¶å†…å®¹
            existing_content = base64.b64decode(existing_data['content']).decode('utf-8')
            return {
                'sha': existing_data['sha'],
                'content': existing_content,
                'exists': True
            }
        else:
            return {'exists': False}
    except:
        return {'exists': False}


def should_update_file(new_content, existing_info):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ–‡ä»¶"""
    if not existing_info['exists']:
        return True

    # æ¯”è¾ƒå†…å®¹å“ˆå¸Œ
    new_hash = get_file_content_hash(new_content)
    existing_hash = get_file_content_hash(existing_info['content'])

    return new_hash != existing_hash


def add_file_to_batch(folder_name, filename, content):
    """å°†æ–‡ä»¶æ·»åŠ åˆ°æ‰¹é‡æäº¤åˆ—è¡¨"""
    file_path = f"{GITHUB_PATH}/{folder_name}/{filename}.md"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦éœ€è¦æ›´æ–°
    existing_info = get_existing_file_info(file_path)

    if should_update_file(content, existing_info):
        file_info = {
            'path': file_path,
            'content': content,
            'folder_name': folder_name,
            'filename': filename,
            'sha': existing_info.get('sha') if existing_info['exists'] else None,
            'is_new': not existing_info['exists']
        }
        pending_files.append(file_info)
        print(f"ğŸ“ å¾…æ›´æ–°: {folder_name}/{filename}.md")
        return True
    else:
        return False


def commit_files_batch():
    """æ‰¹é‡æäº¤æ‰€æœ‰å¾…æ›´æ–°çš„æ–‡ä»¶ - å•æ¬¡æäº¤"""
    if not pending_files:
        print("ğŸ“„ æ²¡æœ‰æ–‡ä»¶éœ€è¦æ›´æ–°")
        return 0

    print(f"\nğŸš€ å¼€å§‹å•æ¬¡æ‰¹é‡æäº¤ {len(pending_files)} ä¸ªæ–‡ä»¶...")

    headers = {
        'Authorization': f'token {GITHUB_TOKEN}',
        'Accept': 'application/vnd.github.v3+json'
    }

    # è·å–ä»“åº“ä¿¡æ¯å’Œé»˜è®¤åˆ†æ”¯
    repo_url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}'
    try:
        repo_response = requests.get(repo_url, headers=headers)
        repo_response.raise_for_status()
        default_branch = repo_response.json()['default_branch']
        print(f"ğŸŒ¿ æ£€æµ‹åˆ°é»˜è®¤åˆ†æ”¯: {default_branch}")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è·å–ä»“åº“ä¿¡æ¯: {e}")
        print(f"ğŸ”„ å›é€€åˆ°å…¼å®¹æ¨¡å¼...")
        return commit_files_individually()

    try:
        # 1. è·å–å½“å‰åˆ†æ”¯çš„æœ€æ–°commit
        ref_url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/refs/heads/{default_branch}'
        ref_response = requests.get(ref_url, headers=headers)
        ref_response.raise_for_status()
        base_commit_sha = ref_response.json()['object']['sha']
        print(f"ğŸ“ å½“å‰åˆ†æ”¯æœ€æ–°commit: {base_commit_sha[:8]}")

        # 2. è·å–åŸºç¡€tree
        commit_url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/commits/{base_commit_sha}'
        commit_response = requests.get(commit_url, headers=headers)
        commit_response.raise_for_status()
        base_tree_sha = commit_response.json()['tree']['sha']
        print(f"ğŸ“ åŸºç¡€tree: {base_tree_sha[:8]}")

        # 3. å‡†å¤‡tree entries
        tree_entries = []
        new_files = []
        updated_files = []

        for file_info in pending_files:
            # åˆ›å»ºblob
            blob_data = {
                'content': file_info['content'],
                'encoding': 'utf-8'
            }
            
            blob_url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/blobs'
            blob_response = requests.post(blob_url, headers=headers, json=blob_data)
            blob_response.raise_for_status()
            blob_sha = blob_response.json()['sha']

            # æ·»åŠ åˆ°tree entries
            tree_entries.append({
                'path': file_info['path'],
                'mode': '100644',
                'type': 'blob',
                'sha': blob_sha
            })

            if file_info['is_new']:
                new_files.append(file_info)
            else:
                updated_files.append(file_info)

        print(f"ğŸ“¦ åˆ›å»ºäº† {len(tree_entries)} ä¸ªblobå¯¹è±¡")

        # 4. åˆ›å»ºæ–°tree
        tree_data = {
            'base_tree': base_tree_sha,
            'tree': tree_entries
        }

        tree_url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/trees'
        tree_response = requests.post(tree_url, headers=headers, json=tree_data)
        tree_response.raise_for_status()
        new_tree_sha = tree_response.json()['sha']
        print(f"ğŸŒ³ åˆ›å»ºæ–°tree: {new_tree_sha[:8]}")

        # 5. ç”Ÿæˆcommit message
        commit_message = f"ğŸ”„ NotionåŒæ­¥ - æ‰¹é‡æ›´æ–° {len(pending_files)} ä¸ªæ–‡ä»¶"

        if new_files:
            commit_message += f"\n\nâœ¨ æ–°å¢ {len(new_files)} ä¸ªæ–‡ä»¶:"
            for file_info in new_files:
                commit_message += f"\n  + {file_info['folder_name']}/{file_info['filename']}.md"
        
        if updated_files:
            commit_message += f"\n\nğŸ“ æ›´æ–° {len(updated_files)} ä¸ªæ–‡ä»¶:"
            for file_info in updated_files:
                commit_message += f"\n  ğŸ“„ {file_info['folder_name']}/{file_info['filename']}.md"

        # 6. åˆ›å»ºcommit
        commit_data = {
            'message': commit_message,
            'tree': new_tree_sha,
            'parents': [base_commit_sha]
        }

        commit_create_url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/git/commits'
        commit_create_response = requests.post(commit_create_url, headers=headers, json=commit_data)
        commit_create_response.raise_for_status()
        new_commit_sha = commit_create_response.json()['sha']
        print(f"ğŸ’¾ åˆ›å»ºæ–°commit: {new_commit_sha[:8]}")

        # 7. æ›´æ–°åˆ†æ”¯å¼•ç”¨
        ref_update_data = {
            'sha': new_commit_sha
        }

        ref_update_response = requests.patch(ref_url, headers=headers, json=ref_update_data)
        ref_update_response.raise_for_status()
        print(f"ğŸ¯ æ›´æ–°åˆ†æ”¯å¼•ç”¨æˆåŠŸ")

        print(f"âœ… å•æ¬¡æ‰¹é‡æäº¤å®Œæˆ! æˆåŠŸæäº¤ {len(pending_files)} ä¸ªæ–‡ä»¶åˆ°ä¸€ä¸ªcommitä¸­")
        return len(pending_files)

    except Exception as e:
        print(f"âŒ å•æ¬¡æ‰¹é‡æäº¤å¤±è´¥: {e}")
        print(f"ğŸ”„ å›é€€åˆ°å…¼å®¹æ¨¡å¼...")
        return commit_files_individually()


def commit_files_individually():
    """å•ä¸ªæ–‡ä»¶æäº¤ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    print("ğŸ”„ ä½¿ç”¨å…¼å®¹æ¨¡å¼ï¼ˆæ¯ä¸ªæ–‡ä»¶å•ç‹¬æäº¤ï¼‰...")

    success_count = 0
    headers = {
        'Authorization': f'token {GITHUB_TOKEN}',
        'Accept': 'application/vnd.github.v3+json'
    }

    for file_info in pending_files:
        url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{file_info["path"]}'

        # é‡æ–°è·å–æœ€æ–°çš„SHAä»¥é¿å…å†²çª
        try:
            check_response = requests.get(url, headers=headers)
            if check_response.status_code == 200:
                current_file = check_response.json()
                current_sha = current_file['sha']

                # æ£€æŸ¥å†…å®¹æ˜¯å¦çœŸçš„ä¸åŒ
                current_content = base64.b64decode(current_file['content']).decode('utf-8')
                if get_file_content_hash(current_content) == get_file_content_hash(file_info['content']):
                    continue
            else:
                current_sha = None
        except:
            current_sha = file_info.get('sha')

        encoded_content = base64.b64encode(file_info['content'].encode('utf-8')).decode('utf-8')

        data = {
            'message': f'æ›´æ–°ç¬”è®°: {file_info["folder_name"]}/{file_info["filename"]}',
            'content': encoded_content
        }

        if current_sha:
            data['sha'] = current_sha

        try:
            response = requests.put(url, headers=headers, json=data)
            response.raise_for_status()
            print(f"âœ… å•ç‹¬æäº¤: {file_info['folder_name']}/{file_info['filename']}.md")
            success_count += 1
        except requests.exceptions.RequestException as e:
            if "409" in str(e):
                print(f"âš ï¸ æ–‡ä»¶å†²çªï¼Œè·³è¿‡: {file_info['folder_name']}/{file_info['filename']}.md")
            else:
                print(f"âŒ æäº¤å¤±è´¥: {file_info['folder_name']}/{file_info['filename']}.md - {e}")

    return success_count


def clean_filename(filename):
    """æ¸…ç†æ–‡ä»¶å"""
    # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•çš„æ–‡ä»¶å­—ç¬¦
    invalid_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
    for char in invalid_chars:
        filename = filename.replace(char, '_')

    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    filename = filename.strip(' .')

    # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
    filename = filename.replace(' ', '_')

    # å¦‚æœæ–‡ä»¶åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
    if not filename:
        filename = "æœªå‘½åé¡µé¢"

    return filename


def process_database(database_info, db_index, total_dbs, file_mapping, database_page_ids=None):
    """å¤„ç†å•ä¸ªæ•°æ®åº“"""
    database_id = database_info['id']
    database_title = database_info['title']

    print(f"\nğŸ“š æ­£åœ¨å¤„ç†æ•°æ®åº“ {db_index}/{total_dbs}: {database_title}")

    # è·å–æ•°æ®åº“ä¸­çš„ç¬”è®°
    notes_data = fetch_notion_notes(database_id)
    if not notes_data:
        print(f"âŒ æ— æ³•è·å–æ•°æ®åº“ {database_id} çš„ç¬”è®°")
        return 0

    # å¤„ç†æ¯ä¸ªé¡µé¢
    pages = notes_data.get('results', [])
    print(f"ğŸ“„ æ‰¾åˆ° {len(pages)} ä¸ªé¡µé¢")

    processed_count = 0
    folder_stats = {}  # ç»Ÿè®¡å„ä¸ªæ–‡ä»¶å¤¹çš„æ–‡ä»¶æ•°é‡
    
    for page in pages:
        page_id = page['id']
        
        # æ”¶é›†é¡µé¢IDç”¨äºç‹¬ç«‹é¡µé¢å»é‡
        if database_page_ids is not None:
            database_page_ids.add(page_id)

        # è·å–é¡µé¢å±æ€§
        page_properties = get_page_properties(page)
        
        # ç”Ÿæˆæ–‡ä»¶å
        title = get_page_title(page)
        if not title:
            title = f"é¡µé¢_{page_id}"
        
        # ç”Ÿæˆæ–‡ä»¶å¤¹è·¯å¾„
        folder_path = generate_folder_path(database_title, page_properties)
        
        # ç»Ÿè®¡æ–‡ä»¶å¤¹
        if folder_path not in folder_stats:
            folder_stats[folder_path] = 0
        folder_stats[folder_path] += 1

        # è·å–é¡µé¢å†…å®¹
        content_data = get_page_content(page_id)

        # è½¬æ¢ä¸ºMarkdown
        source_info = f"æ•°æ®åº“: {database_title}"
        markdown_content = convert_notion_to_markdown(page, content_data, source_info)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆtitleå·²ç»åœ¨å‰é¢è·å–äº†ï¼‰
        filename = clean_filename(title)
        new_file_path = f"{GITHUB_PATH}/{folder_path}/{filename}.md"

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤æ—§ä½ç½®çš„æ–‡ä»¶
        if page_id in file_mapping:
            old_file_path = file_mapping[page_id]
            if old_file_path != new_file_path:
                print(f"ğŸ”„ æ£€æµ‹åˆ°æ–‡ä»¶ä½ç½®å˜æ›´: {page_id}")
                print(f"   æ—§ä½ç½®: {old_file_path}")
                print(f"   æ–°ä½ç½®: {new_file_path}")
                # åˆ é™¤æ—§ä½ç½®çš„æ–‡ä»¶
                delete_github_file(old_file_path)

        # æ›´æ–°æ˜ å°„è¡¨
        file_mapping[page_id] = new_file_path

        # æ·»åŠ åˆ°æ‰¹é‡æäº¤åˆ—è¡¨æˆ–ç«‹å³ä¿å­˜
        if BATCH_COMMIT:
            if add_file_to_batch(folder_path, filename, markdown_content):
                print(f"   ğŸ“„ {title} -> {folder_path}")
                processed_count += 1
        else:
            if save_to_github_immediate(folder_path, filename, markdown_content):
                print(f"   ğŸ“„ {title} -> {folder_path}")
                processed_count += 1

    # æ˜¾ç¤ºç»Ÿè®¡
    if folder_stats:
        print(f"   ğŸ“ {len(folder_stats)} ä¸ªæ–‡ä»¶å¤¹ï¼Œ{processed_count}/{len(pages)} ä¸ªé¡µé¢éœ€è¦åŒæ­¥")
    else:
        print(f"   âœ… {processed_count}/{len(pages)} ä¸ªé¡µé¢éœ€è¦åŒæ­¥")
    return processed_count


def save_to_github_immediate(folder_name, filename, content):
    """ç«‹å³ä¿å­˜åˆ°GitHubï¼ˆæ—§æ–¹å¼ï¼Œä¿æŒå…¼å®¹ï¼‰"""
    headers = {
        'Authorization': f'token {GITHUB_TOKEN}',
        'Accept': 'application/vnd.github.v3+json'
    }

    # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«æ–‡ä»¶å¤¹ç»“æ„
    file_path = f"{GITHUB_PATH}/{folder_name}/{filename}.md"
    url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{file_path}'

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    try:
        existing_response = requests.get(url, headers=headers)
        if existing_response.status_code == 200:
            existing_data = existing_response.json()
            sha = existing_data['sha']
        else:
            sha = None
    except:
        sha = None

    # ç¼–ç å†…å®¹
    encoded_content = base64.b64encode(content.encode('utf-8')).decode('utf-8')

    # å‡†å¤‡æ•°æ®
    data = {
        'message': f'æ›´æ–°ç¬”è®°: {folder_name}/{filename}',
        'content': encoded_content
    }

    if sha:
        data['sha'] = sha

    try:
        response = requests.put(url, headers=headers, json=data)
        response.raise_for_status()
        print(f"âœ… æˆåŠŸä¿å­˜æ–‡ä»¶: {folder_name}/{filename}.md")
        return True
    except requests.exceptions.RequestException as e:
        print(f"âŒ ä¿å­˜åˆ°GitHubæ—¶å‡ºé”™: {e}")
        return False


def process_standalone_pages(file_mapping, database_page_ids=None):
    """å¤„ç†ç‹¬ç«‹é¡µé¢ï¼ˆä¸åœ¨æ•°æ®åº“ä¸­çš„é¡µé¢ï¼‰"""
    print(f"\nğŸ“„ æ­£åœ¨æœç´¢æ‰€æœ‰ç‹¬ç«‹é¡µé¢...")

    # è·å–æ‰€æœ‰é¡µé¢
    all_pages = search_all_pages()
    print(f"ğŸ” æ‰¾åˆ° {len(all_pages)} ä¸ªé¡µé¢")

    # å¦‚æœæ²¡æœ‰ä¼ å…¥æ•°æ®åº“é¡µé¢IDï¼Œåˆ™è·å–
    if database_page_ids is None:
        database_ids = get_database_ids()
        database_page_ids = set()
        for database_id in database_ids:
            notes_data = fetch_notion_notes(database_id)
            if notes_data and 'results' in notes_data:
                for page in notes_data['results']:
                    database_page_ids.add(page['id'])

    print(f"ğŸ—‚ï¸ æ•°æ®åº“ä¸­å…±æœ‰ {len(database_page_ids)} ä¸ªé¡µé¢")

    # è¿‡æ»¤å‡ºçœŸæ­£çš„ç‹¬ç«‹é¡µé¢
    standalone_pages = []
    for page in all_pages:
        page_id = page['id']
        
        # è·³è¿‡å·²ç»åœ¨æˆ‘ä»¬é…ç½®çš„æ•°æ®åº“ä¸­çš„é¡µé¢
        if page_id in database_page_ids:
            continue
            
        parent = page.get('parent', {})
        
        # åªå¤„ç†å·¥ä½œåŒºæ ¹é¡µé¢æˆ–ä¸åœ¨ä»»ä½•æ•°æ®åº“ä¸­çš„é¡µé¢
        if parent.get('type') in ['workspace'] or (
            parent.get('type') == 'page_id' and 
            parent.get('page_id') not in database_page_ids
        ):
            standalone_pages.append(page)

    print(f"ğŸ“‘ æ‰¾åˆ° {len(standalone_pages)} ä¸ªçœŸæ­£çš„ç‹¬ç«‹é¡µé¢")

    if not standalone_pages:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°ç‹¬ç«‹é¡µé¢")
        return 0

    # å¤„ç†ç‹¬ç«‹é¡µé¢
    processed_count = 0
    folder_stats = {}  # ç»Ÿè®¡å„ä¸ªæ–‡ä»¶å¤¹çš„æ–‡ä»¶æ•°é‡

    for i, page in enumerate(standalone_pages, 1):
        page_id = page['id']
        title = get_page_title(page)

        if not title:
            title = f"é¡µé¢_{page_id[:8]}"

        # è·å–é¡µé¢å±æ€§ï¼ˆç‹¬ç«‹é¡µé¢ä¹Ÿå¯èƒ½æœ‰å±æ€§ï¼‰
        page_properties = get_page_properties(page)
        
        # ä½¿ç”¨é¡µé¢æ ‡é¢˜ä½œä¸ºåŸºç¡€æ–‡ä»¶å¤¹ï¼Œå°±åƒæ•°æ®åº“æ ‡é¢˜ä¸€æ ·
        page_folder = generate_folder_path(title, page_properties)
        
        # ç»Ÿè®¡æ–‡ä»¶å¤¹
        if page_folder not in folder_stats:
            folder_stats[page_folder] = 0
        folder_stats[page_folder] += 1

        # è·å–é¡µé¢å†…å®¹
        content_data = get_page_content(page_id)

        # è½¬æ¢ä¸ºMarkdown
        source_info = f"ç‹¬ç«‹é¡µé¢: {title}"
        markdown_content = convert_notion_to_markdown(page, content_data, source_info)

        # æ–‡ä»¶åä½¿ç”¨å›ºå®šåç§°ï¼Œå› ä¸ºæ–‡ä»¶å¤¹å·²ç»æ˜¯é¡µé¢åç§°äº†
        filename = "content"  # æˆ–è€…å¯ä»¥ä½¿ç”¨é¡µé¢æ ‡é¢˜ï¼Œä½†å¯èƒ½ä¼šé‡å¤
        new_file_path = f"{GITHUB_PATH}/{page_folder}/{filename}.md"

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤æ—§ä½ç½®çš„æ–‡ä»¶
        if page_id in file_mapping:
            old_file_path = file_mapping[page_id]
            if old_file_path != new_file_path:
                print(f"ğŸ”„ æ£€æµ‹åˆ°ç‹¬ç«‹é¡µé¢ä½ç½®å˜æ›´: {page_id}")
                print(f"   æ—§ä½ç½®: {old_file_path}")
                print(f"   æ–°ä½ç½®: {new_file_path}")
                # åˆ é™¤æ—§ä½ç½®çš„æ–‡ä»¶
                delete_github_file(old_file_path)

        # æ›´æ–°æ˜ å°„è¡¨
        file_mapping[page_id] = new_file_path

        # æ·»åŠ åˆ°æ‰¹é‡æäº¤åˆ—è¡¨æˆ–ç«‹å³ä¿å­˜
        if BATCH_COMMIT:
            if add_file_to_batch(page_folder, filename, markdown_content):
                print(f"   ğŸ“„ {title} -> {page_folder}")
                processed_count += 1
        else:
            if save_to_github_immediate(page_folder, filename, markdown_content):
                print(f"   ğŸ“„ {title} -> {page_folder}")
                processed_count += 1
    
    # æ˜¾ç¤ºç»Ÿè®¡
    if folder_stats:
        print(f"   ğŸ“ {len(folder_stats)} ä¸ªæ–‡ä»¶å¤¹ï¼Œ{processed_count}/{len(standalone_pages)} ä¸ªé¡µé¢éœ€è¦åŒæ­¥")
    else:
        print(f"   âœ… {processed_count}/{len(standalone_pages)} ä¸ªé¡µé¢éœ€è¦åŒæ­¥")
    return processed_count


def check_github_repo_status():
    """æ£€æŸ¥GitHubä»“åº“çŠ¶æ€å’Œåˆ†æ”¯ä¿¡æ¯"""
    headers = {
        'Authorization': f'token {GITHUB_TOKEN}',
        'Accept': 'application/vnd.github.v3+json'
    }

    # æ£€æŸ¥ä»“åº“æ˜¯å¦å­˜åœ¨
    repo_url = f'https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}'
    try:
        repo_response = requests.get(repo_url, headers=headers)
        if repo_response.status_code == 404:
            print(f"âŒ ä»“åº“ä¸å­˜åœ¨: {GITHUB_OWNER}/{GITHUB_REPO}")
            print(f"ğŸ’¡ è¯·ç¡®è®¤ï¼š")
            print(f"   - GITHUB_OWNER: {GITHUB_OWNER}")
            print(f"   - GITHUB_REPO: {GITHUB_REPO}")
            print(f"   - ä»“åº“æ˜¯å¦å­˜åœ¨ä¸”æœ‰æƒé™è®¿é—®")
            return False
        repo_response.raise_for_status()

        repo_data = repo_response.json()
        default_branch = repo_data['default_branch']

        print(f"âœ… ä»“åº“æ£€æŸ¥é€šè¿‡: {GITHUB_OWNER}/{GITHUB_REPO}")
        print(f"ğŸŒ¿ é»˜è®¤åˆ†æ”¯: {default_branch}")
        print(f"ğŸ”’ ä»“åº“ç±»å‹: {'ç§æœ‰' if repo_data['private'] else 'å…¬å¼€'}")

        return True

    except requests.exceptions.RequestException as e:
        if "401" in str(e):
            print(f"âŒ GitHub Tokenæƒé™ä¸è¶³")
            print(f"ğŸ’¡ è¯·æ£€æŸ¥ï¼š")
            print(f"   - GITHUB_TOKENæ˜¯å¦æ­£ç¡®")
            print(f"   - Tokenæ˜¯å¦æœ‰ä»“åº“è®¿é—®æƒé™")
        else:
            print(f"âŒ ä»“åº“æ£€æŸ¥å¤±è´¥: {e}")
        return False


def sync_notion_to_github():
    """ä¸»åŒæ­¥å‡½æ•°"""
    global pending_files
    pending_files = []  # é‡ç½®å¾…æäº¤æ–‡ä»¶åˆ—è¡¨

    print("ğŸš€ å¼€å§‹åŒæ­¥Notionå†…å®¹åˆ°GitHub...")
    print(f"ğŸ”§ åŒæ­¥æ¨¡å¼: {SYNC_MODE}")
    print(f"ğŸ“¦ æ‰¹é‡æäº¤: {'å¼€å¯' if BATCH_COMMIT else 'å…³é—­'}")
    print(f"ğŸš« è·³è¿‡æäº¤: {'æ˜¯' if SKIP_COMMIT else 'å¦'}")
    print(f"ğŸ“‚ æ–‡ä»¶å¤¹åˆ†ç±»: {'å¼€å¯' if ENABLE_CATEGORIZATION else 'å…³é—­'}")
    if ENABLE_CATEGORIZATION:
        print(f"ğŸ·ï¸ åˆ†ç±»å±æ€§: {', '.join(CATEGORY_PROPERTIES)}")
    else:
        print("âš ï¸ æ–‡ä»¶å¤¹åˆ†ç±»å·²ç¦ç”¨ï¼Œæ‰€æœ‰æ–‡ä»¶å°†æ”¾åœ¨æ•°æ®åº“åŒåæ–‡ä»¶å¤¹ä¸‹")

    # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
    if not all([NOTION_API_KEY, GITHUB_TOKEN, GITHUB_REPO, GITHUB_OWNER]):
        print("âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡")
        return

    # æ£€æŸ¥GitHubä»“åº“çŠ¶æ€
    print(f"\nğŸ” æ£€æŸ¥GitHubä»“åº“çŠ¶æ€...")
    if not check_github_repo_status():
        print("âŒ GitHubä»“åº“æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®åé‡è¯•")
        return

    # åŠ è½½æ–‡ä»¶ä½ç½®æ˜ å°„è¡¨
    print(f"ğŸ“‹ åŠ è½½æ–‡ä»¶ä½ç½®æ˜ å°„è¡¨...")
    file_mapping = load_file_mapping()
    print(f"ğŸ“Š å½“å‰è·Ÿè¸ª {len(file_mapping)} ä¸ªæ–‡ä»¶ä½ç½®")

    total_processed = 0
    database_page_ids = set()  # æ”¶é›†æ•°æ®åº“é¡µé¢IDï¼Œç”¨äºç‹¬ç«‹é¡µé¢å»é‡

    # åŒæ­¥æ•°æ®åº“
    if SYNC_MODE in ['databases', 'all']:
        database_ids = get_database_ids()

        if database_ids:
            print(f"\nğŸ“Š æ‰¾åˆ° {len(database_ids)} ä¸ªæ•°æ®åº“è¦åŒæ­¥")

            # è·å–æ‰€æœ‰æ•°æ®åº“ä¿¡æ¯
            database_infos = []
            for i, database_id in enumerate(database_ids, 1):
                print(f"ğŸ” è·å–æ•°æ®åº“ {i}/{len(database_ids)} ä¿¡æ¯...")
                db_info = get_database_info(database_id)
                database_infos.append(db_info)
                print(f"  ğŸ“‹ {db_info['title']}")

            # å¤„ç†æ¯ä¸ªæ•°æ®åº“
            for i, database_info in enumerate(database_infos, 1):
                processed_count = process_database(database_info, i, len(database_infos), file_mapping, database_page_ids)
                total_processed += processed_count
        else:
            print("âš ï¸ æ²¡æœ‰é…ç½®æ•°æ®åº“IDï¼Œè·³è¿‡æ•°æ®åº“åŒæ­¥")

    # åŒæ­¥ç‹¬ç«‹é¡µé¢
    if SYNC_MODE in ['pages', 'all']:
        standalone_processed = process_standalone_pages(file_mapping, database_page_ids)
        total_processed += standalone_processed

    # æ¸…ç†å·²åˆ é™¤é¡µé¢çš„æ–‡ä»¶
    clean_deleted_pages(file_mapping)

    # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶ä½ç½®æ˜ å°„è¡¨
    print(f"\nğŸ’¾ ä¿å­˜æ–‡ä»¶ä½ç½®æ˜ å°„è¡¨...")
    save_file_mapping(file_mapping)
    print(f"ğŸ“Š å½“å‰è·Ÿè¸ª {len(file_mapping)} ä¸ªæ–‡ä»¶ä½ç½®")

    # æ‰§è¡Œæ‰¹é‡æäº¤
    if SKIP_COMMIT:
        print(f"\nâ­ï¸ è·³è¿‡æäº¤æ­¥éª¤ï¼Œå…±å‡†å¤‡äº† {len(pending_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ’¡ å¦‚éœ€æäº¤ï¼Œè¯·è®¾ç½® SKIP_COMMIT=false é‡æ–°è¿è¡Œ")
    elif BATCH_COMMIT and pending_files:
        committed_count = commit_files_batch()
        if committed_count > 0:
            print(f"\nğŸ‰ åŒæ­¥å®Œæˆ! æ‰€æœ‰ {committed_count} ä¸ªæ–‡ä»¶å·²åˆå¹¶åˆ°ä¸€æ¬¡æäº¤ä¸­")
            print(f"ğŸ“Š æ‰¹é‡æäº¤ï¼š{len(pending_files)} ä¸ªæ–‡ä»¶ = 1 ä¸ªcommit")
        else:
            print(f"\nâŒ æ‰¹é‡æäº¤å¤±è´¥ï¼Œå·²ä½¿ç”¨å…¼å®¹æ¨¡å¼")
    elif not BATCH_COMMIT and not SKIP_COMMIT:
        committed_count = commit_files_individually()
        print(f"\nğŸ‰ åŒæ­¥å®Œæˆ! ä½¿ç”¨å…¼å®¹æ¨¡å¼æäº¤äº† {committed_count} ä¸ªæ–‡ä»¶")
    else:
        print(f"\nğŸ‰ åŒæ­¥å®Œæˆ! æ²¡æœ‰æ–‡ä»¶éœ€è¦æ›´æ–°")

    print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°GitHubçš„ {GITHUB_PATH} æ–‡ä»¶å¤¹ä¸‹")
    print(f"ğŸ“‚ æ–‡ä»¶å¤¹ç»“æ„: æ•°æ®åº“æ–‡ä»¶å¤¹ + ç‹¬ç«‹é¡µé¢æ–‡ä»¶å¤¹")


if __name__ == '__main__':
    sync_notion_to_github()